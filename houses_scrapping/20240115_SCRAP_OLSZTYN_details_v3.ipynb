{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf283ed7",
   "metadata": {},
   "source": [
    "# OLSZTYN\n",
    "- scraping details \n",
    "- DATE 2024 jan 16th - this is done using URLs downloaded on the 16th of January"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a617cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x20297a53a48>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# prepering directories\n",
    "path = os.getcwd()\n",
    "# subfolders\n",
    "input_dir = os.path.join(path, 'inputs')\n",
    "output_dir = os.path.join(path, 'outputs')\n",
    "\n",
    "# open folder\n",
    "subprocess.Popen(f'explorer \"{output_dir}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23714de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "C:\\Users\\Ukasz\\Anaconda3\\envs\\geopandas\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "# selenium \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d756e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'olsztyn_page_all_pages_20240116170027.csv'\n",
    "date = datetime.datetime.now().strftime(\"%Y%m%d\") # current date\n",
    "new_file_name = f'{date}_olsztyn_details_test.csv'\n",
    "df = pd.read_csv(os.path.join(os.getcwd(), 'outputs', data), index_col=False)\n",
    "df = df.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605dceb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chrome driver location\n",
    "PATH = os.path.join(path, 'inputs','chromedriver.exe' )\n",
    "\n",
    "# chrom driver configuration\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument('--ignore-certificate-errors')\n",
    "options.add_argument('--incognito')\n",
    "#options.add_argument('--headless') # without opening browser\n",
    "\n",
    "driver = webdriver.Chrome(PATH, options=options)\n",
    "\n",
    "wait = WebDriverWait(driver, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9bb8613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url \n",
    "url = 'https://www.otodom.pl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6347ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0...1...2...3...4...5...6...7...8...9...10...11...12...13...14...15...16...17...18...19...20...21...22...23...24...25...26...27...28...29...30...31...32...33...34...35...36...37...38...39...DONE !! \n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "this piece of code adds \n",
    "uses a while loop and writes the data not found to expired or not_scraped sets\n",
    "'''\n",
    "\n",
    "not_scraped = set()\n",
    "expired_set = set()\n",
    "to_do = set(list(df.index))\n",
    "\n",
    "top_tags = ['Powierzchnia','Forma własności','Liczba pokoi','Stan wykończenia','Piętro','Balkon / ogród / taras','Czynsz','Miejsce parkingowe','Obsługa zdalna','Ogrzewanie']\n",
    "bottom_tags = ['Rynek', 'Typ ogłoszeniodawcy', 'Dostępne od','Rok budowy','Rodzaj zabudowy','Okna','Winda','Media','Zabezpieczenia','Wyposażenie','Informacje dodatkowe','Materiał budynku']\n",
    "\n",
    "def accept_terms():\n",
    "    # accepting terms and conditions\n",
    "    # this has to be done only once\n",
    "    element = wait.until(EC.element_to_be_clickable((By.ID, 'onetrust-pc-btn-handler')))\n",
    "    element.click()\n",
    "    # step two -  confirming preferences and closing the modal window\n",
    "    element = wait.until(EC.element_to_be_clickable((By.XPATH, \"//*[contains(text(), 'Potwierdzenie moich wyborów')]\")))\n",
    "    element.click()\n",
    "    \n",
    "    \n",
    "def get_data(soup):\n",
    "    data = {}\n",
    "    top_information = soup.find('div', attrs={'data-testid':'ad.top-information.table'})\n",
    "    bottom_information = soup.find('div', attrs={'data-testid':'ad.additional-information.table'})\n",
    "       \n",
    "    for tag in top_tags:\n",
    "        try:\n",
    "            div_tag = top_information.find('div', attrs={'aria-label':tag})\n",
    "            results = div_tag.find_all(\"div\")\n",
    "            data[tag] = results[2].text\n",
    "        except:\n",
    "            #data[tag] = results[2].text\n",
    "            data[tag] = 'brak informacji'\n",
    "            \n",
    "    for tag in bottom_tags:\n",
    "        try:\n",
    "            div_tag = bottom_information.find('div', attrs={'aria-label':tag})\n",
    "            results = div_tag.find_all(\"div\")\n",
    "            data[tag] = results[2].text\n",
    "        except:\n",
    "            #data[tag] = results[2].text\n",
    "            data[tag] = 'brak informacji'\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def save_data(i, data, out_df, date):\n",
    "    \n",
    "    for k in data:\n",
    "        out_df.loc[i, k] = data[k]\n",
    "    out_df.to_csv(os.path.join(path, 'outputs', new_file_name), encoding='utf-8', index=False)\n",
    "\n",
    "    \n",
    "expired = 0\n",
    "while list(not_scraped.union(to_do)):\n",
    "    for i in list(df.index):\n",
    "        date = datetime.datetime.now().strftime(\"%Y%m%d\") # current date\n",
    "        link = df['add_link'][i]\n",
    "        # open website using webdriver\n",
    "        driver.get(url + link)\n",
    "        if i == 0:\n",
    "            out_df = df.copy()\n",
    "            out_df.to_csv(os.path.join(path, 'outputs', new_file_name), encoding='utf-8', index=False)\n",
    "            accept_terms()\n",
    "\n",
    "        time.sleep(2) # waits n seconds  \n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);var lenOfPage=document.body.scrollHeight;return lenOfPage;\")\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # expired add\n",
    "        if soup.findAll(text=\"To ogłoszenie jest już niedostępne\"):\n",
    "            expired += 1\n",
    "            print(f'{i}_Expired', end='...')\n",
    "            expired_set.add(i)\n",
    "            to_do.remove(i)\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # data available in soup,  all as it should be\n",
    "            data = get_data(soup)\n",
    "            if i > 0:\n",
    "                out_df = pd.read_csv(os.path.join(os.getcwd(), 'outputs', new_file_name), encoding='utf-8', index_col=False)   \n",
    "            save_data(i, data, out_df, date)\n",
    "            # remove id from to_do or from not_scraped\n",
    "            if i in not_scraped:\n",
    "                not_scraped.remove(i)\n",
    "            if i in to_do:\n",
    "                to_do.remove(i)\n",
    "            print(i, end='...')\n",
    "        except:\n",
    "            # was not able to scrap data so id is added to not_scraped\n",
    "            not_scraped.add(i)\n",
    "            print(f'{i} Issue', end='...')\n",
    "\n",
    "print('DONE !! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b881ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
